{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "185f6ee3",
   "metadata": {},
   "source": [
    "# Why Residual Connections?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44a42b",
   "metadata": {},
   "source": [
    "With the advent of the Transformer architecture ([Attention is All You Need](https://arxiv.org/abs/1706.03762)) both the original implementation and modern day implementations use residual connections to improve performance and training speed.\n",
    "\n",
    "The hitch is that many ML researchers today don't know where this idea came from or erroneously attribute it to the transformer architecture. My goal in this notebook is to explain what residual connections are, when they should be used, and broadly why they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a2b396",
   "metadata": {},
   "source": [
    "# What are Residual Connections?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e191ea8a",
   "metadata": {},
   "source": [
    "![alt text](transformer_red.png \"Transformer Architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899cde6d",
   "metadata": {},
   "source": [
    "# MNIST Model Implemented Fast with fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50161db7",
   "metadata": {},
   "source": [
    "## Define the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d4bcc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<fastai.data.load.DataLoader at 0x7f45304a5a90>,\n",
       " <fastai.data.load.DataLoader at 0x7f4530473250>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.vision.all import * # Usually imports everything needed for computer vision tasks in fastai\n",
    "import os\n",
    "import torchvision\n",
    "\n",
    "assert os.path.exists('/mnt/2tb-drive/data'), \"Data directory does not exist. Please check the path.\"\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "trainmnist = torchvision.datasets.MNIST('/mnt/2tb-drive/data/MNIST', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(dataset=trainmnist, batch_size=32, shuffle=True, num_workers=16) # fastai's DataLoader\n",
    "testmnist = torchvision.datasets.MNIST('/mnt/2tb-drive/data/MNIST', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(dataset=testmnist, batch_size=32, shuffle=False, num_workers=16) # fastai's DataLoader\n",
    "\n",
    "trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c229d087",
   "metadata": {},
   "source": [
    "## Residual Networks are Just Fine for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "942901ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.117826</td>\n",
       "      <td>0.110482</td>\n",
       "      <td>0.968400</td>\n",
       "      <td>03:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.084140</td>\n",
       "      <td>0.040583</td>\n",
       "      <td>0.987000</td>\n",
       "      <td>03:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.062278</td>\n",
       "      <td>0.047078</td>\n",
       "      <td>0.986500</td>\n",
       "      <td>03:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.048858</td>\n",
       "      <td>0.031450</td>\n",
       "      <td>0.990300</td>\n",
       "      <td>03:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = DataLoaders(trainloader, testloader)\n",
    "dls.c = 10  # Set number of classes for MNIST\n",
    "# https://docs.fast.ai/vision.learner.html#vision_learner\n",
    "l = vision_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy, n_in=1)\n",
    "l.fit(4, 1e-3)  # Train for 4 epochs with a learning rate of 0.01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
