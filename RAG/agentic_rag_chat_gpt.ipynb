{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a58510a",
   "metadata": {},
   "source": [
    "# Agentic RAG Chatbot with OpenAI Assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09546d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openai API key successfully imported from .env file.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "else:\n",
    "  print(f\"Openai API key successfully imported from .env file.\")\n",
    "client = OpenAI()\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"YOUR ASSISTANT NAME HERE\",\n",
    "    description=\"YOUR DESC HERE\",\n",
    "    instructions=\"YOUR INSTRUCTIONS HERE\",\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\"type\": \"file_search\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6ac689",
   "metadata": {},
   "source": [
    "Now we have an *assistant* defined, we just need to include our vector database / files so that our assistant can have the context it needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00dacb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n"
     ]
    }
   ],
   "source": [
    "vector_store = client.vector_stores.create(name=\"VECTOR STORE NAME HERE\")\n",
    "\n",
    "# Ready the files for upload to OpenAI\n",
    "file_paths = [\"Example_data/Prompt_Engineering.pdf\"] # example file path, replace with your own files\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = client.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    "\n",
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d42a34",
   "metadata": {},
   "source": [
    "Now we have files and an assistant lets allow the assistant access to the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90df6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d89145",
   "metadata": {},
   "source": [
    "Now its time to ask our assistant questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b78fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45476/3424891763.py:7: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
      "  thread = client.beta.threads.create(\n",
      "/tmp/ipykernel_45476/3424891763.py:20: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
      "  run = client.beta.threads.runs.create_and_poll(\n",
      "/tmp/ipykernel_45476/3424891763.py:24: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
      "  messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 3 best ways to prompt a large language model (LLM) include:\n",
      "\n",
      "1. **Few-shot prompting**: This technique involves providing the model with a few example inputs and outputs (usually three to five) relevant to the task. These examples should be of high quality and well-written to reduce model confusion and improve the output's reliability[0].\n",
      "\n",
      "2. **Chain of Thought (CoT) prompting**: This method encourages the model to generate intermediate reasoning steps before arriving at a final answer. It helps improve the model's reasoning capabilities and is especially useful for complex tasks that require logical steps[1].\n",
      "\n",
      "3. **System, Contextual, and Role prompting**:\n",
      "   - **System prompting**: Sets the overall context and purpose for the model, defining what it should be doing.\n",
      "   - **Contextual prompting**: Provides specific details or background relevant to the task, helping the model understand and tailor responses more accurately.\n",
      "   - **Role prompting**: Assigns a specific identity or character to the model, guiding it to generate responses consistent with that role[2][3][4]. \n",
      "\n",
      "Using these techniques ensures more accurate and contextually relevant outputs from LLMs.\n",
      "[0] Prompt_Engineering.pdf\n",
      "[1] Prompt_Engineering.pdf\n",
      "[2] Prompt_Engineering.pdf\n",
      "[3] Prompt_Engineering.pdf\n",
      "[4] Prompt_Engineering.pdf\n"
     ]
    }
   ],
   "source": [
    "# Upload more documents if needed on the fly\n",
    "#message_file = client.files.create(\n",
    "#  file=open(\"edgar/aapl-10k.pdf\", \"rb\"), purpose=\"assistants\"\n",
    "#)\n",
    "\n",
    "# Create a thread and attach the file to the message\n",
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What are the 3 best ways to prompt a LLM?\",\n",
    "      # Attach the new file to the message.\n",
    "      #\"attachments\": [\n",
    "      #  { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "      #],\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "message_content = messages[0].content[0].text\n",
    "annotations = message_content.annotations\n",
    "citations = []\n",
    "for index, annotation in enumerate(annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "    if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "        cited_file = client.files.retrieve(file_citation.file_id)\n",
    "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "print(message_content.value)\n",
    "print(\"\\n\".join(citations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab655550",
   "metadata": {},
   "source": [
    "# Responses API - Cleaner More Concise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b03e39",
   "metadata": {},
   "source": [
    "What if we wanted to do the same thing as above, but lighterwieght and easier to implement? Enter the OpenAI Responses API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6784cb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key successfully imported from .env file.\n",
      "Here are three effective ways to prompt a Large Language Model (LLM):\n",
      "\n",
      "1. **Chain of Thought (CoT) Prompting**: This technique involves structuring prompts to include intermediate reasoning steps. It improves the model's ability to generate accurate and logical responses by guiding it through the thought process needed to solve complex tasks.\n",
      "\n",
      "2. **Role and Contextual Prompting**: Assigning a specific role to the LLM and providing contextual information helps the model generate more relevant and accurate outputs. Role prompting gives the model a perspective or position to adopt, while contextual prompting supplies background knowledge relevant to the task.\n",
      "\n",
      "3. **Be Specific and Simple**: Prompts should be clear, concise, and avoid unnecessary complexity. Clearly specify the desired output, and use action-oriented verbs to direct the model's response effectively.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import getpass\n",
    "import os\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "else:\n",
    "    print(f\"OpenAI API key successfully imported from .env file.\")\n",
    "client = OpenAI()\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"What are the 3 best ways to prompt a LLM?\",\n",
    "    tools=[{\"type\": \"file_search\", \"vector_store_ids\": [vector_store.id]}],\n",
    ")\n",
    "#print(response.to_json())\n",
    "print(response.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
