{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4053c29a",
   "metadata": {},
   "source": [
    "# RAG with OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e43ba91",
   "metadata": {},
   "source": [
    "## Load OpenAI Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "client = OpenAI()\n",
    "vector_store = client.vector_stores.create(name=\"VECTOR STORE NAME HERE\")\n",
    "\n",
    "# Ready the files for upload to OpenAI\n",
    "file_paths = [\"Example_data/Prompt_Engineering.pdf\"] # example file path, replace with your own files\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = client.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    "\n",
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b5f285",
   "metadata": {},
   "source": [
    "## Ask questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71221d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"What are the 3 best ways to prompt a LLM?\",\n",
    "    tools=[{\"type\": \"file_search\", \"vector_store_ids\": [vector_store.id]}],\n",
    ")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995f10af",
   "metadata": {},
   "source": [
    "# RAG with Langchain and Custom Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ee5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "import getpass\n",
    "import os\n",
    "import dotenv\n",
    "# Load environment variables from .env file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "# we assume you have a vector store set up\n",
    "# Replace YOUR_VECTOR_STORE with your actual vector store instance or allow openai to create one in the next chapter\n",
    "vector_store = YOUR_VECTOR_STORE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddcf70f",
   "metadata": {},
   "source": [
    "# Retrieval and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict, Tuple\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "## Prompt - custom\n",
    "template = \"\"\"Use the following pieces of context to answer the question given by the USER. \n",
    "Be concise and accurate in your response. Admit if you do not know the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "{chat_history}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\", \"chat_history\": \"(messages go here)\"}\n",
    ").to_messages()\n",
    "\n",
    "## State and Nodes\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    chat_history: List[Tuple[str, str]]\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    # Convert the chat_history tuples to BaseMessage objects\n",
    "    history_messages = []\n",
    "    for human, ai in state[\"chat_history\"]:\n",
    "        history_messages.append(HumanMessage(content=human))\n",
    "        history_messages.append(AIMessage(content=ai))\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content, \"chat_history\": history_messages})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "## Compile the graph\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6bb800",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f16002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.state import CompiledStateGraph\n",
    "class ChatBotWithMemory():\n",
    "    def __init__(self, graph: CompiledStateGraph, chat_history: List[Tuple[str, str]]=[]):\n",
    "        self.graph = graph\n",
    "        self.chat_history = chat_history\n",
    "    \n",
    "    def ask_question(self, question: str):\n",
    "        state = self.graph.invoke({\"question\": question, \"chat_history\": self.chat_history})\n",
    "        self.chat_history.append((question, state[\"answer\"]))\n",
    "        return state[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376f63cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = ChatBotWithMemory(graph)\n",
    "question = \"How tall is the eiffel tower?\"\n",
    "print(chatbot.ask_question(question))\n",
    "question = \"What is the capital of France?\"\n",
    "print(chatbot.ask_question(question))\n",
    "question = \"Name a city there.\"\n",
    "print(chatbot.ask_question(question))\n",
    "question = \"Name another city.\"\n",
    "print(chatbot.ask_question(question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
